{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnpr1VPKUwY9"
      },
      "source": [
        "Integrantes:   \n",
        "- Daniel Carmona\n",
        "- Consuelo Rojas\n",
        "\n",
        "En esta tarea van a crear una red neuronal que clasifique mensajes como spam o no spam. Lo primero es descargar la data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ZkXXLb0VUmFX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2022-11-29 16:04:20--  https://www.ivan-sipiran.com/downloads/spam.csv\n",
            "Resolving www.ivan-sipiran.com (www.ivan-sipiran.com)... 66.96.149.31\n",
            "Connecting to www.ivan-sipiran.com (www.ivan-sipiran.com)|66.96.149.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471781 (461K)\n",
            "Saving to: 'spam.csv.6'\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 10%  304K 1s\n",
            "    50K .......... .......... .......... .......... .......... 21%  345K 1s\n",
            "   100K .......... .......... .......... .......... .......... 32% 2.70M 1s\n",
            "   150K .......... .......... .......... .......... .......... 43%  462K 1s\n",
            "   200K .......... .......... .......... .......... .......... 54% 1.53M 0s\n",
            "   250K .......... .......... .......... .......... .......... 65% 4.16M 0s\n",
            "   300K .......... .......... .......... .......... .......... 75% 5.51M 0s\n",
            "   350K .......... .......... .......... .......... .......... 86% 7.27M 0s\n",
            "   400K .......... .......... .......... .......... .......... 97%  432K 0s\n",
            "   450K ..........                                            100%  160M=0.6s\n",
            "\n",
            "2022-11-29 16:04:22 (754 KB/s) - 'spam.csv.6' saved [471781/471781]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.ivan-sipiran.com/downloads/spam.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9567d6oVknI"
      },
      "source": [
        "Los datos vienen en un archivo CSV que contiene dos columnas \"text\" y \"label\". La columna \"text\" contiene el texto del mensaje y la columna \"label\" contiene las etiquetas \"ham\" y \"spam\". Un mensaje \"ham\" es un mensaje que no se considera spam.\n",
        "\n",
        "# Tarea \n",
        "El objetivo de la tarea es crear una red neuronal que clasifique los datos entregados. Para lograr esto debes:\n",
        "\n",
        "\n",
        "\n",
        "*   Implementar el pre-procesamiento de los datos que creas necesario.\n",
        "*   Particionar los datos en 70% entrenamiento, 10% validación y 20% test.\n",
        "*   Usa los datos de entrenamiento y valiadación para tus experimentos y sólo usa el conjunto de test para reportar el resultado final.\n",
        "\n",
        "Para el diseño de la red neuronal puedes usar una red neuronal recurrente o una red basada en transformers. El objetivo de la tarea no es obtener el performance ultra máximo, sino entender qué decisiones de diseño afectan la solución de un problema como este. Lo que si es necesario (como siempre) es que discutas los resultados y decisiones realizadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5pmzo1gcVXvY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize  \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Pre-procesamiento\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Clasifación\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre-procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  Go until jurong point, crazy.. Available only ...   ham\n",
              "1                      Ok lar... Joking wif u oni...   ham\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
              "3  U dun say so early hor... U c already then say...   ham\n",
              "4  Nah I don't think he goes to usf, he lives aro...   ham"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv('spam.csv').dropna() #, encoding='latin-1')\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ham' 'spam' '#&gt' 'URL&gt' 'DECIMAL&gt' 'l 09064012103 box334sk38ch'\n",
            " '  &lt' ':(' ')'\n",
            " ' they wer askd 2 sit in an aeroplane. Aftr they sat they wer told dat the plane ws made by their students. Dey all hurried out of d plane.. Bt only 1 didnt move... He said:\\\\if it is made by my students'\n",
            " 'TIME&gt' ' successful day.' ' wish U Merry Xmas...'\n",
            " \" have to reach before 4'o clock so call me plz\" ' sweet dreams'\n",
            " ' HAVE A NICE SLEEP..SWEET DREAMS..'\n",
            " ' best friends.. GOODEVENING Dear..:)' \" don't worry.\"\n",
            " ' Finally It Becomes Part Of Your Life.. Follow It.. Happy Morning &amp'\n",
            " \"i'm in luv wid u. Blue\" 'Take care' 'abel'\n",
            " ') reminds me i still need 2go.did u c d little thing i left in the lounge?'\n",
            " \" it kills me that u don't care enough to stop me...\" '_'\n",
            " ' send this to ur frndZ &amp' ' fletcher now'\n",
            " ' I love u Grl: Hogolo Boy: gold chain kodstini Grl: Agalla Boy: necklace madstini Grl: agalla Boy: Hogli 1 mutai eerulli kodthini! Grl: I love U kano'\n",
            " '-) Good morning.. keep smiling:-)' ') None? People hate u:'\n",
            " ' Life is empty without frnds.. So Alwys Be In Touch. Good night &amp'\n",
            " ').' ' HAVE A SMILEY SUNDAY..:)' '-)'\n",
            " ' u dont tell me......... Take care:-)' ' If i dont make it hav gd night'\n",
            " '3' \" You didn't see my facebook huh?\" ' get up before 3'\n",
            " ' present number..:) By Rajitha Raj (Ranju)'\n",
            " '  express your love to someone....that much it will hurt when they leave you or you get seperated...!\\x8eö´\\x89Ó_??\\x8bÛ¬ud evening...'\n",
            " \" fowler now but I'm in my mom's car so I can't park (long story)\"\n",
            " ' Make happy the person you love. In the same way friendship has one law']\n"
          ]
        }
      ],
      "source": [
        "print(df[\"label\"].unique())\n",
        "# print(df[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ham     4617\n",
            "spam     746\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df2 = df.apply(lambda row: row[df['label'].isin([\"ham\", \"spam\"])])\n",
        "print(df2[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos algunas stopword que queremos que sean eliminadas\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Definimos un tokenizador con Stemming\n",
        "class StemmerTokenizer:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "    def __call__(self, doc):\n",
        "        doc_tok = word_tokenize(doc)\n",
        "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
        "        return [self.ps.stem(t) for t in doc_tok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "bow = CountVectorizer(tokenizer= StemmerTokenizer(), ngram_range=(1,2))\n",
        "df_bow = bow.fit_transform(df2[\"text\"])\n",
        "\n",
        "df_bow = pd.DataFrame(df_bow.toarray(), columns=bow.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>37960</th>\n",
              "      <th>37961</th>\n",
              "      <th>37962</th>\n",
              "      <th>37963</th>\n",
              "      <th>37964</th>\n",
              "      <th>37965</th>\n",
              "      <th>37966</th>\n",
              "      <th>37967</th>\n",
              "      <th>37968</th>\n",
              "      <th>37969</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 37970 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
              "0      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "1      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "2      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "4      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "\n",
              "   37960  37961  37962  37963  37964  37965  37966  37967  37968  37969  \n",
              "0      0      0      0      0      0      0      0      0      0      0  \n",
              "1      0      0      0      0      0      0      0      0      0      0  \n",
              "2      0      0      0      0      0      0      0      0      0      0  \n",
              "3      0      0      0      0      0      0      0      0      0      0  \n",
              "4      0      0      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 37970 columns]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_bow = SelectPercentile(f_classif, percentile=90).fit_transform(df_bow, df2[\"label\"])\n",
        "df_bow = pd.DataFrame(df_bow)\n",
        "df_bow.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: \n",
            " ham     3245\n",
            "spam     508\n",
            "Name: label, dtype: int64\n",
            "val: \n",
            " ham     453\n",
            "spam     84\n",
            "Name: label, dtype: int64\n",
            "test: \n",
            " ham     919\n",
            "spam    154\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_bow, df2[\"label\"], test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1) # 0.125 x 0.8 = 0.1\n",
        "\n",
        "print(\"train: \\n\", y_train.value_counts())\n",
        "print(\"val: \\n\", y_val.value_counts())\n",
        "print(\"test: \\n\", y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Esta función permite inicializar todas las semillas de números pseudoaleatorios.\n",
        "Puedes usar esta función para resetear los generadores de números aleatorios'''\n",
        "def iniciar_semillas():\n",
        "  SEED = 1234\n",
        "\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#Función para computar el accuracy. Se asume que predicciones y etiquetas son tensores en el GPU\n",
        "def calculate_accuracy(y_pred, y):\n",
        "  top_pred = y_pred.argmax(1, keepdim=True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float()/y.shape[0]\n",
        "  return acc\n",
        "\n",
        "'''Función para entrenar una época de un modelo. Recibe como parámetros\n",
        "    -model: una red neuronal\n",
        "    -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "    -optimizer: el optimizador para el entrenamiento\n",
        "    -criterion: la función de loss\n",
        "    -device: dispositivo a usar para el entrenamiento\n",
        "\n",
        "Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)'''\n",
        "def train_one_epoch(model, iterator, optimizer, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We have to set the neural network in training mode. This is because during\n",
        "  #training, we need gradients and complementary data to ease the computation  \n",
        "  model.train()\n",
        "  \n",
        "  #Training loop\n",
        "  for (x, y) in iterator:\n",
        "    print(iterator)\n",
        "    print(x, y)\n",
        "    x = x.to(device) #Data\n",
        "    y = y.long().to(device) #Labels\n",
        "        \n",
        "    optimizer.zero_grad() #Clean gradients\n",
        "             \n",
        "    y_pred = model(x) #Feed the network with data\n",
        "        \n",
        "    loss = criterion(y_pred, y) #Compute the loss\n",
        "       \n",
        "    acc = calculate_accuracy(y_pred, y) #Compute the accuracy\n",
        "        \n",
        "    loss.backward() #Compute gradients\n",
        "        \n",
        "    optimizer.step() #Apply update rules\n",
        "        \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "        \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "'''Función que evalúa una red neuronal con un conjunto de datos de prueba. Recibe como parámetros\n",
        "    -model: una red neuronal\n",
        "    -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "    -criterion: la función de loss\n",
        "    -device: dispositivo a usar para el entrenamiento\n",
        "Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)'''\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We put the network in testing mode\n",
        "  #In this mode, Pytorch doesn't use features only reserved for \n",
        "  #training (dropout for instance)    \n",
        "  model.eval()\n",
        "    \n",
        "  with torch.no_grad(): #disable the autograd engine (save computation and memory)\n",
        "        \n",
        "    for (x, y) in iterator:\n",
        "      x = x.to(device)\n",
        "      y = y.long().to(device)\n",
        "\n",
        "      y_pred= model(x)\n",
        "\n",
        "      loss = criterion(y_pred, y)\n",
        "\n",
        "      acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "#Calcula el tiempo transcurrido entre dos timestamps\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "'''Esta función realiza el entrenamiento completo de una red. Recibe como parámetros:\n",
        "    -network: la red neuronal\n",
        "    -optimizer: el optimizador para entrenamiento\n",
        "    -train_loader: el dataloader de datos de entrenamiento\n",
        "    -tes_loader: el dataloader de datos de prueba\n",
        "    -name: nombre a usar para guardar en disco la red con el mejor accuracy'''\n",
        "def train_complete(network, device, optimizer, train_loader, val_loader, test_loader, name, epochs=10):\n",
        "  \n",
        "  #Se envían la red y la función de loss al GPU\n",
        "  network = network.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  #Fijar el entrenamiento en 20 épocas siempre\n",
        "  EPOCHS = epochs\n",
        "\n",
        "  best_valid_acc = float('-inf')\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    #Train + validation cycles  \n",
        "    train_loss, train_acc = train_one_epoch(network, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(network, val_loader, criterion, device)\n",
        "    \n",
        "    #Si encontramos un modelo con accuracy de validación mayor, lo guardamos\n",
        "    if valid_acc > best_valid_acc:\n",
        "     best_valid_acc = valid_acc\n",
        "     torch.save(network.state_dict(), f'{name}.pt')\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "  \n",
        "  #Cuando se termina el entrenamiento, cargamos el mejor modelo guardado y calculamos el accuracy de prueba\n",
        "  network.load_state_dict(torch.load(f'{name}.pt'))\n",
        "\n",
        "  test_loss , test_acc = evaluate(network, test_loader, criterion, device)\n",
        "  print(f'Test Loss: {test_loss:.3f} | Mejor test acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para crear la red debemos heredar desde nn.Module\n",
        "class GruNet(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout): #, \n",
        "                 #pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                    embedding_dim#,\n",
        "                                    #padding_idx=pad_idx,\n",
        "                                    )\n",
        "        # Capa GRU\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout = dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        #self.relu = nn.ReLU()\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Definimos las operaciones de las capas sobre el input en el forward.\n",
        "    def forward(self, text): \n",
        "        embedded = self.embedding(text)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(df_bow.columns)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM\n",
        "OUTPUT_DIM = 2  # número de clases\n",
        "\n",
        "N_LAYERS = 2  # número de capas.\n",
        "DROPOUT = 0.3\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "modelo_3 = GruNet(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT) #, PAD_IDX)\n",
        "\n",
        "model_name_3 = 'Gru_Model'  # nombre que tendrá el modelo guardado...\n",
        "n_epochs_3 = 10\n",
        "\n",
        "\n",
        "# Loss: Cross Entropy\n",
        "# TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "# criterion_3 = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyper-parameters \n",
        "batch_size = 32\n",
        "train_data = {\"data\": X_train.to_numpy(), \"label\": y_train.to_numpy()}\n",
        "val_data = {\"data\": X_val.to_numpy(), \"label\": y_val.to_numpy()}\n",
        "test_data = {\"data\": X_test.to_numpy(), \"label\": y_test.to_numpy()}\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "momentum = 0.9\n",
        "log_interval = 100\n",
        "learning_rate=0.01"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "32f2fe102a4f10662d8c13f75131e1ba377b7194060421a642fdea27c55fc65a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
