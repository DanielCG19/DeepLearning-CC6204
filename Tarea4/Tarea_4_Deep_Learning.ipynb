{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnpr1VPKUwY9"
      },
      "source": [
        "Integrantes:   \n",
        "- Daniel Carmona\n",
        "- Consuelo Rojas\n",
        "\n",
        "En esta tarea van a crear una red neuronal que clasifique mensajes como spam o no spam. Lo primero es descargar la data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZkXXLb0VUmFX"
      },
      "outputs": [],
      "source": [
        "# !wget https://www.ivan-sipiran.com/downloads/spam.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9567d6oVknI"
      },
      "source": [
        "Los datos vienen en un archivo CSV que contiene dos columnas \"text\" y \"label\". La columna \"text\" contiene el texto del mensaje y la columna \"label\" contiene las etiquetas \"ham\" y \"spam\". Un mensaje \"ham\" es un mensaje que no se considera spam.\n",
        "\n",
        "# Tarea \n",
        "El objetivo de la tarea es crear una red neuronal que clasifique los datos entregados. Para lograr esto debes:\n",
        "\n",
        "\n",
        "\n",
        "*   Implementar el pre-procesamiento de los datos que creas necesario.\n",
        "*   Particionar los datos en 70% entrenamiento, 10% validación y 20% test.\n",
        "*   Usa los datos de entrenamiento y valiadación para tus experimentos y sólo usa el conjunto de test para reportar el resultado final.\n",
        "\n",
        "Para el diseño de la red neuronal puedes usar una red neuronal recurrente o una red basada en transformers. El objetivo de la tarea no es obtener el performance ultra máximo, sino entender qué decisiones de diseño afectan la solución de un problema como este. Lo que si es necesario (como siempre) es que discutas los resultados y decisiones realizadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5pmzo1gcVXvY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize  \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Pre-procesamiento\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Modelos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre-procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  Go until jurong point, crazy.. Available only ...   ham\n",
              "1                      Ok lar... Joking wif u oni...   ham\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
              "3  U dun say so early hor... U c already then say...   ham\n",
              "4  Nah I don't think he goes to usf, he lives aro...   ham"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv('spam.csv').dropna() #, encoding='latin-1')\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ham' 'spam' '#&gt' 'URL&gt' 'DECIMAL&gt' 'l 09064012103 box334sk38ch'\n",
            " '  &lt' ':(' ')'\n",
            " ' they wer askd 2 sit in an aeroplane. Aftr they sat they wer told dat the plane ws made by their students. Dey all hurried out of d plane.. Bt only 1 didnt move... He said:\\\\if it is made by my students'\n",
            " 'TIME&gt' ' successful day.' ' wish U Merry Xmas...'\n",
            " \" have to reach before 4'o clock so call me plz\" ' sweet dreams'\n",
            " ' HAVE A NICE SLEEP..SWEET DREAMS..'\n",
            " ' best friends.. GOODEVENING Dear..:)' \" don't worry.\"\n",
            " ' Finally It Becomes Part Of Your Life.. Follow It.. Happy Morning &amp'\n",
            " \"i'm in luv wid u. Blue\" 'Take care' 'abel'\n",
            " ') reminds me i still need 2go.did u c d little thing i left in the lounge?'\n",
            " \" it kills me that u don't care enough to stop me...\" '_'\n",
            " ' send this to ur frndZ &amp' ' fletcher now'\n",
            " ' I love u Grl: Hogolo Boy: gold chain kodstini Grl: Agalla Boy: necklace madstini Grl: agalla Boy: Hogli 1 mutai eerulli kodthini! Grl: I love U kano'\n",
            " '-) Good morning.. keep smiling:-)' ') None? People hate u:'\n",
            " ' Life is empty without frnds.. So Alwys Be In Touch. Good night &amp'\n",
            " ').' ' HAVE A SMILEY SUNDAY..:)' '-)'\n",
            " ' u dont tell me......... Take care:-)' ' If i dont make it hav gd night'\n",
            " '3' \" You didn't see my facebook huh?\" ' get up before 3'\n",
            " ' present number..:) By Rajitha Raj (Ranju)'\n",
            " '  express your love to someone....that much it will hurt when they leave you or you get seperated...!\\x8eö´\\x89Ó_??\\x8bÛ¬ud evening...'\n",
            " \" fowler now but I'm in my mom's car so I can't park (long story)\"\n",
            " ' Make happy the person you love. In the same way friendship has one law']\n"
          ]
        }
      ],
      "source": [
        "print(df[\"label\"].unique())\n",
        "# print(df[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ham     4617\n",
            "spam     746\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df2 = df.apply(lambda row: row[df['label'].isin([\"ham\", \"spam\"])])\n",
        "print(df2[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos algunas stopword que queremos que sean eliminadas\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Definimos un tokenizador con Stemming\n",
        "class StemmerTokenizer:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "    def __call__(self, doc):\n",
        "        doc_tok = word_tokenize(doc)\n",
        "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
        "        return [self.ps.stem(t) for t in doc_tok]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "bow = CountVectorizer(tokenizer= StemmerTokenizer(), ngram_range=(1,2))\n",
        "df_bow = bow.fit_transform(df2[\"text\"])\n",
        "\n",
        "df_bow = pd.DataFrame(df_bow.toarray(), columns=bow.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>29522</th>\n",
              "      <th>29523</th>\n",
              "      <th>29524</th>\n",
              "      <th>29525</th>\n",
              "      <th>29526</th>\n",
              "      <th>29527</th>\n",
              "      <th>29528</th>\n",
              "      <th>29529</th>\n",
              "      <th>29530</th>\n",
              "      <th>29531</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29532 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
              "0      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "1      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "2      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "4      0      0      0      0      0      0      0      0      0      0  ...   \n",
              "\n",
              "   29522  29523  29524  29525  29526  29527  29528  29529  29530  29531  \n",
              "0      0      0      0      0      0      0      0      0      0      0  \n",
              "1      0      0      0      0      0      0      0      0      0      0  \n",
              "2      0      0      0      0      0      0      0      0      0      0  \n",
              "3      0      0      0      0      0      0      0      0      0      0  \n",
              "4      0      0      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 29532 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_bow = SelectPercentile(f_classif, percentile=70).fit_transform(df_bow, df2[\"label\"])\n",
        "df_bow = pd.DataFrame(df_bow)\n",
        "df_bow.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: \n",
            " ham     3245\n",
            "spam     508\n",
            "Name: label, dtype: int64\n",
            "val: \n",
            " ham     453\n",
            "spam     84\n",
            "Name: label, dtype: int64\n",
            "test: \n",
            " ham     919\n",
            "spam    154\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_bow, df2[\"label\"], test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1) # 0.125 x 0.8 = 0.1\n",
        "\n",
        "print(\"train: \\n\", y_train.value_counts())\n",
        "print(\"val: \\n\", y_val.value_counts())\n",
        "print(\"test: \\n\", y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Esta función permite inicializar todas las semillas de números pseudoaleatorios.\n",
        "Puedes usar esta función para resetear los generadores de números aleatorios'''\n",
        "def iniciar_semillas():\n",
        "  SEED = 1234\n",
        "\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#Función para computar el accuracy. Se asume que predicciones y etiquetas son tensores en el GPU\n",
        "def calculate_accuracy(y_pred, y):\n",
        "  top_pred = y_pred.argmax(1, keepdim=True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float()/y.shape[0]\n",
        "  return acc\n",
        "\n",
        "'''Función para entrenar una época de un modelo. Recibe como parámetros\n",
        "    -model: una red neuronal\n",
        "    -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "    -optimizer: el optimizador para el entrenamiento\n",
        "    -criterion: la función de loss\n",
        "    -device: dispositivo a usar para el entrenamiento\n",
        "\n",
        "Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)'''\n",
        "def train_one_epoch(model, iterator, optimizer, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We have to set the neural network in training mode. This is because during\n",
        "  #training, we need gradients and complementary data to ease the computation  \n",
        "  model.train()\n",
        "  batch_size = 15 #x.size(0)\n",
        "  #hidden_state = model.init_hidden(batch_size)\n",
        "  #Training loop\n",
        "  for idx, data in enumerate(iterator):\n",
        "  #for (x, y) in iterator:\n",
        "    # print(iterator)\n",
        "    # print(x, y)\n",
        "    x, y = data[0].to(torch.int64), data[1].type(torch.LongTensor)  #data[0].to(torch.float32), data[1].to(torch.float32)\n",
        "    x = x.to(device)#.float() #Data\n",
        "    y = y.to(device)#.long() #Labels\n",
        "    hidden_state = model.init_hidden(x.shape[0])\n",
        "    hidden_state = tuple([each.data for each in hidden_state])\n",
        "    #hidden_state = tuple([each.repeat(1, batch_size, 1).data for each in hidden_state]) \n",
        "        \n",
        "    optimizer.zero_grad() #Clean gradients  \n",
        "    y_pred, hidden_state = model(x, hidden_state)#.to(torch.int64)      #model(x.squeeze(), hidden_state)    \n",
        "    #y_pred = torch.transpose(model(x), 1, 2) #Feed the network with data    \n",
        "    loss = criterion(y_pred, y) #Compute the loss   \n",
        "    acc = calculate_accuracy(y_pred, y) #Compute the accuracy \n",
        "    #hidden_state = hidden_state.detach()\n",
        "         \n",
        "    loss.backward()#retain_graph=True) #Compute gradients  \n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 5)  \n",
        "    optimizer.step() #Apply update rules\n",
        "        \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "        \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "'''Función que evalúa una red neuronal con un conjunto de datos de prueba. Recibe como parámetros\n",
        "    -model: una red neuronal\n",
        "    -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "    -criterion: la función de loss\n",
        "    -device: dispositivo a usar para el entrenamiento\n",
        "Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)'''\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We put the network in testing mode\n",
        "  #In this mode, Pytorch doesn't use features only reserved for \n",
        "  #training (dropout for instance)    \n",
        "  model.eval()\n",
        "    \n",
        "  with torch.no_grad(): #disable the autograd engine (save computation and memory)\n",
        "\n",
        "    batch_size = 15 #y.size(0)\n",
        "    #hidden_state = model.init_hidden(batch_size)    \n",
        "    #for (x, y) in iterator:\n",
        "    for idx, data in enumerate(iterator):\n",
        "      \n",
        "      #hidden_state = model.init_hidden(15)\n",
        "      x, y = data[0].to(torch.int64), data[1].type(torch.LongTensor)  #data[0].to(torch.float32), data[1].to(torch.float32)\n",
        "      x = x.to(device)#.float() #Data\n",
        "      y = y.to(device)#.long() #Labels\n",
        "      hidden_state = model.init_hidden(x.shape[0]) #batch_size)\n",
        "      hidden_state = tuple([each.data for each in hidden_state])\n",
        "      #hidden_state = tuple([each.repeat(1, batch_size, 1).data for each in hidden_state])  \n",
        "      y_pred, hidden_state = model(x, hidden_state) #torch.argmax(model(x))\n",
        "  \n",
        "      #y_pred = torch.transpose(model(x), 1, 2) #Feed the network with data \n",
        "      loss = criterion(y_pred.squeeze(), y)\n",
        "      acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "#Calcula el tiempo transcurrido entre dos timestamps\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "'''Esta función realiza el entrenamiento completo de una red. Recibe como parámetros:\n",
        "    -network: la red neuronal\n",
        "    -optimizer: el optimizador para entrenamiento\n",
        "    -train_loader: el dataloader de datos de entrenamiento\n",
        "    -tes_loader: el dataloader de datos de prueba\n",
        "    -name: nombre a usar para guardar en disco la red con el mejor accuracy'''\n",
        "def train_complete(network, device, optimizer, criterion, train_loader, val_loader, test_loader, name, epochs=10):\n",
        "  \n",
        "  #Se envían la red y la función de loss al GPU\n",
        "  network = network.to(device)\n",
        "  #criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  #Fijar el entrenamiento en 20 épocas siempre\n",
        "  EPOCHS = epochs\n",
        "\n",
        "  best_valid_acc = float('-inf')\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    #Train + validation cycles  \n",
        "    train_loss, train_acc = train_one_epoch(network, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(network, val_loader, criterion, device)\n",
        "    \n",
        "    #Si encontramos un modelo con accuracy de validación mayor, lo guardamos\n",
        "    if valid_acc > best_valid_acc:\n",
        "     best_valid_acc = valid_acc\n",
        "     torch.save(network.state_dict(), f'{name}.pt')\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "  \n",
        "  #Cuando se termina el entrenamiento, cargamos el mejor modelo guardado y calculamos el accuracy de prueba\n",
        "  network.load_state_dict(torch.load(f'{name}.pt'))\n",
        "\n",
        "  test_loss , test_acc = evaluate(network, test_loader, criterion, device)\n",
        "  print(f'Test Loss: {test_loss:.3f} | Mejor test acc: {test_acc*100:.2f}%')\n",
        "  return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "#ist(le.inverse_transform(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyper-parameters \n",
        "batch_size = 15\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "tensor_x_train = torch.Tensor(X_train.to_numpy()) # transform to torch tensor\n",
        "tensor_y_train = torch.Tensor(y_train)\n",
        "train_dataset = TensorDataset(tensor_x_train,tensor_y_train) # create your datset\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False) # create your dataloader\n",
        "\n",
        "tensor_x_val = torch.Tensor(X_val.to_numpy()) # transform to torch tensor\n",
        "tensor_y_val = torch.Tensor(y_val)\n",
        "val_dataset = TensorDataset(tensor_x_val,tensor_y_val) # create your datset\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False) # create your dataloader\n",
        "\n",
        "tensor_x_test = torch.Tensor(X_test.to_numpy()) # transform to torch tensor\n",
        "tensor_y_test = torch.Tensor(y_test)\n",
        "test_dataset = TensorDataset(tensor_x_test,tensor_y_test) # create your datset\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # create your dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Capas embedding y LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # Capa lineal y sigmoide\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "                \n",
        "        #Tomamos solo el último valor de salida del LSTM\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "                \n",
        "        # dropout y fully-connected\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "               \n",
        "        # sigmoide\n",
        "        sig_out = self.sig(out)\n",
        "                  \n",
        "        # retornar sigmoide y último estado oculto\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Crea dos nuevos tensores con tamaño n_layers x batch_size x hidden_dim,\n",
        "        # inicializados a cero, para estado oculto y memoria de LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        # if(train_on_gpu):\n",
        "        #   hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "        #            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        # else:\n",
        "        #   hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "        #            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "CUDA_LAUNCH_BLOCKING=1.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(df_bow.columns) + 1 # +1 for zero padding + our word tokens\n",
        "output_size = 2\n",
        "embedding_dim = 200 #100 \n",
        "hidden_dim = 64 #64\n",
        "n_layers = 2\n",
        "lr = 0.001\n",
        "\n",
        "model_RNN = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "criterion = nn.CrossEntropyLoss()  #nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model_RNN.parameters(), lr=lr, weight_decay=1e-5)\n",
        "#optimizer = torch.optim.SGD(model_RNN.parameters(), lr=lr, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 48s\n",
            "\tTrain Loss: 0.462 | Train Acc: 86.37%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 84.35%\n",
            "Epoch: 02 | Epoch Time: 0m 47s\n",
            "\tTrain Loss: 0.450 | Train Acc: 86.40%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 84.35%\n",
            "Epoch: 03 | Epoch Time: 0m 47s\n",
            "\tTrain Loss: 0.449 | Train Acc: 86.40%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 84.35%\n",
            "Epoch: 04 | Epoch Time: 0m 47s\n",
            "\tTrain Loss: 0.449 | Train Acc: 86.40%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 84.35%\n",
            "Test Loss: 0.458 | Mejor test acc: 85.58%\n"
          ]
        }
      ],
      "source": [
        "model_RNN = train_complete(model_RNN, device, optimizer, criterion, train_loader, val_loader, test_loader, \"model_RNN\", epochs=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
